{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cabamarcos/P1-RRNN/blob/main/P1_RRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![LogoUC3M](data:image/png;base64,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)\n",
        "\n",
        "# PRÁCTICA 1: REDES DE NEURONAS\n",
        "Autores: Pablo Hidalgo Delgado y Marcos Caballero Cortés   \n",
        "\n",
        "NIAs: 100451225 y 100451247\n"
      ],
      "metadata": {
        "id": "EmYCUYAaj0kB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREGUNTAS PROFE\n",
        "\n",
        "\n",
        "1. Exactamente, el error de entrenamiento y validacion que es?? se nos pide guardar la evolucion. Vale para el error de entrenamiento ya que lo calculamos al entrenar el adaline dentro del propio modelo pero para el de validacion?? lo que hacemos es predecir con X_validation y luego calcular el error con y_validation --> solo nos sale un valor, no hay evolucion!\n",
        "Sin embargo, en los de Keras, si hay evolución del error de validacion\n",
        "\n",
        "RESPUESTA: entrenamiento y validacion --> evolucion --> pasar datos de validacion\n",
        "\n",
        "\n",
        "\n",
        "6. Una tabla que contenga los resultados obtenidos para todos los experimentos\n",
        "realizados (hiperparámetros utilizados, errores de entrenamiento, validación y\n",
        "test). --> EL ERROR DE TEST SOLO SERIA PARA EL MEJOR MODELO NO? PARA EL QUE ESCOGEMOS\n",
        "\n",
        "RESPUESTA: error de test para todos los modelos!!\n",
        "\n",
        "EVOLUCION PARA LOS MAS SIGNIFICATIVOS / MEJOR\n",
        "\n",
        "RESULTADOS PARA TODOS ESCOGIENDO EL MENOR ERROR"
      ],
      "metadata": {
        "id": "LAb-DJoc8MBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introducción\n",
        "El propósito de esta práctica es abordar un problema real de regresión utilizando dos modelos de redes de neuronas supervisados:\n",
        "\n",
        "*   El modelo lineal Adaline.\n",
        "*   El modelo no-lineal Perceptrón Multicapa.\n",
        "\n",
        "Para ello, se nos dispone de un conjunto de datos relacionados con medidas realizadas en una turbina de gas a lo largo de 4 años. Debemos predecir el rendimiento energético de la turbina, mostrado en el campo TEY, haciendo uso del resto de mediciones de sensores.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bKPNG-Js-nLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Leer los conjuntos de datos\n"
      ],
      "metadata": {
        "id": "B9zbIk4Jq81r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar, importamos todas las librerías que se van a utilizar a lo largo de la práctica"
      ],
      "metadata": {
        "id": "1a3tpfhSBrhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.table import table\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import load_model\n",
        "\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import json"
      ],
      "metadata": {
        "id": "OKy5cM5JBhxM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leemos los conjuntos de datos, correspondientes a las medidas de 1 solo año, y los juntamos en un mismo dataframe de pandas."
      ],
      "metadata": {
        "id": "DsrHpGp1Bmvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "archivos_csv = ['gt_2011.csv', 'gt_2012.csv', 'gt_2013.csv', 'gt_2014.csv', 'gt_2015.csv']\n",
        "\n",
        "datos = pd.DataFrame()\n",
        "\n",
        "# Concatenamos todos los archivos en un mismo dataframe\n",
        "for archivo in archivos_csv:\n",
        "    # Leemos el archivo\n",
        "    df_temporal = pd.read_csv(archivo)\n",
        "    # Concatenamos el archivo en el dataframe final\n",
        "    datos = pd.concat([datos, df_temporal], ignore_index=True)"
      ],
      "metadata": {
        "id": "xeeZVeeNBbDE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Análisis Exploratorio de Datos (EDA)\n",
        "Antes de empezar con la división en datos de entrenamiento y test, vamos a realizar un pequeño análisis exploratorio de datos (EDA). De esta manera, podremos investigar y resumir las características más importantes de nuestro dataset. Esto nos servirá para comprender mejor los datos y optimizar la obtención de nuestro modelo.\n"
      ],
      "metadata": {
        "id": "xZQ8fVf2CJZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Información básica"
      ],
      "metadata": {
        "id": "ZeDPQVCTVysk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos.info(memory_usage = 'deep')"
      ],
      "metadata": {
        "id": "_WJ_EVupVu52",
        "outputId": "fac1a06c-107f-4f8d-96cf-59d2d8d827fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 36733 entries, 0 to 36732\n",
            "Data columns (total 11 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   AT      36733 non-null  float64\n",
            " 1   AP      36733 non-null  float64\n",
            " 2   AH      36733 non-null  float64\n",
            " 3   AFDP    36733 non-null  float64\n",
            " 4   GTEP    36733 non-null  float64\n",
            " 5   TIT     36733 non-null  float64\n",
            " 6   TAT     36733 non-null  float64\n",
            " 7   TEY     36733 non-null  float64\n",
            " 8   CDP     36733 non-null  float64\n",
            " 9   CO      36733 non-null  float64\n",
            " 10  NOX     36733 non-null  float64\n",
            "dtypes: float64(11)\n",
            "memory usage: 3.1 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El conjunto de datos se compone de 36733 instancias y 11 atributos de tipo float64.\n",
        "\n",
        "También podemos observar que no existen columnas que contengan valores nulos."
      ],
      "metadata": {
        "id": "e3_ymU7Se3rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Atributos compuestos de valores constantes\n",
        "\n",
        "A continuación, vamos a analizar si existen columnas constantes en nuestro conjunto de datos. En ese caso, podremos eliminarlas de nuestro dataset ya que no aportan ninguna información a la variable de respuesta y no existe riesgo de information leakage."
      ],
      "metadata": {
        "id": "MwK44ILqYzQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminamos las columnas con solo un valor único\n",
        "columnas_constantes = []\n",
        "for col in datos.columns:\n",
        "    if datos[col].nunique() == 1:\n",
        "        print(col)\n",
        "        columnas_constantes.append(col)\n",
        "if len(columnas_constantes) == 0:\n",
        "  print(\"No existen atributos con valores constantes\")"
      ],
      "metadata": {
        "id": "psvJsTbCY7nO",
        "outputId": "3a37d5b5-064b-40c1-a787-f7f7073ef8f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No existen atributos con valores constantes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. División de datos en train, validation y test.\n",
        "Una vez explorado y analizado nuestro conjunto de datos, dividimos el dataset en conjuntos de entrenamiento (train), prueba (test) y validación (validation). Para ello, utilizamos la función train_test_split de la librería sklearn. Dividimos los datos en un 70% para train, un 15% para test, y un 15% para validation tal y como se nos pide en el enunciado de la práctica."
      ],
      "metadata": {
        "id": "vkFFsIZce9Kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de realizar la división, debemos separar la variable de respuesta del resto del conjunto de datos."
      ],
      "metadata": {
        "id": "LRpacJZxTqCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = datos.drop(\"TEY\", axis = 1)\n",
        "y = datos[\"TEY\"]"
      ],
      "metadata": {
        "id": "AJERmAn2I8Vq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividimos el conjunto de datos. Lo hacemos especificando el parámetro random_state a 42 para que se aleatorizen los datos y el resultado sea reproducible.\n"
      ],
      "metadata": {
        "id": "okpSS5Nzj54Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividimos los datos originales en conjuntos de entrenamiento (70%) y prueba (30%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40, shuffle = True)\n",
        "\n",
        "# Dividimos el conjunto de prueba en conjuntos de validación (15%) y test (15%)\n",
        "X_validation, X_test, y_validation, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=40, shuffle = True)\n",
        "\n",
        "# Guardamos los conjuntos de datos en archivos CSV\n",
        "X_train.to_csv('X_train.csv', index=False)\n",
        "y_train.to_csv('y_train.csv', index=False)\n",
        "X_validation.to_csv('X_validation.csv', index=False)\n",
        "y_validation.to_csv('y_validation.csv', index=False)\n",
        "X_test.to_csv('X_test.csv', index=False)\n",
        "y_test.to_csv('y_test.csv', index=False)"
      ],
      "metadata": {
        "id": "lRxgG08Zj5L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Preproceso\n",
        "Previamente a entrenar los modelos, debemos realizar una transformación de los datos para que puedan ser interpretados por el algoritmo de manera eficiente. Las transformaciones de datos que realizamos son las siguientes:"
      ],
      "metadata": {
        "id": "Nh7BY9vsyJg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Normalización\n",
        "Para que no se produzca information leakage y no dar información al conjunto de test, normalizamos todos los datos con el mínimo y máximo obtenido del conjunto de entrenamiento."
      ],
      "metadata": {
        "id": "nI1wNR0fyy2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for columna in X_train:\n",
        "  # Obtenemos el minimo y el maximo de cada columna del conjunto de train\n",
        "  min = X_train[columna].min()\n",
        "  max = X_train[columna].max()\n",
        "  # Conjunto de entrenamiento\n",
        "  X_train[columna] = (X_train[columna] - min)/(max - min)\n",
        "  # Conjunto de validación\n",
        "  X_validation[columna] = (X_validation[columna] - min)/(max - min)\n",
        "  # Conjunto de test\n",
        "  X_test[columna] = (X_test[columna] - min)/(max - min)\n",
        "\n",
        "y_min = y_train.min()\n",
        "y_max = y_train.max()\n",
        "# Conjunto de entrenamiento\n",
        "y_train = (y_train - y_min)/(y_max - y_min)\n",
        "# Conjunto de validación\n",
        "y_validation = (y_validation - y_min)/(y_max - y_min)\n",
        "# Conjunto de test\n",
        "y_test = (y_test - y_min)/(y_max - y_min)"
      ],
      "metadata": {
        "id": "cbyqklL9hkjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Aleatorización\n",
        "La aleatorización de los datos la hemos realizado al dividir el conjunto de datos en train, test y validation al especificar el parametro shuffle = True."
      ],
      "metadata": {
        "id": "PYbn2KoGveBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Modelo lineal Adaline"
      ],
      "metadata": {
        "id": "GtpQaB4wj562"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Implementación"
      ],
      "metadata": {
        "id": "grlDOjTxfIuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Adaline_for:\n",
        "    def __init__(self, learning_rate=0.1, epochs=50, random_seed = 45):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.random_seed = random_seed\n",
        "        self.error_entrenamiento = []\n",
        "        self.error_validation = []\n",
        "        self.best_epoch = {\n",
        "                          'MSE Validation': 100000000, # Para que al comparar el error de valdiación en la primera iteración siempre sea menor\n",
        "                          'Best Epoch': 0\n",
        "                          }\n",
        "\n",
        "    def fit(self, X, y, X_validation, y_validation):\n",
        "      # Inicializamos el Adaline\n",
        "        self.__initialize_Adaline(X)\n",
        "\n",
        "        # Convertimos los argumentos a arrays de numpy\n",
        "        X = X.values\n",
        "        y = y.to_numpy().reshape(-1, 1)\n",
        "        X_validation = X_validation.values\n",
        "        y_validation = y_validation.to_numpy().reshape(-1, 1)\n",
        "\n",
        "        # Para cada época\n",
        "        for i in range(self.epochs):\n",
        "          error_filas = [] # Error de entrenamiento de cada entrada\n",
        "          error_filas_validation = [] # Error de validación de cada entrada\n",
        "\n",
        "          # Para cada fila del conjunto de entrenamiento\n",
        "          for fila in range(X.shape[0]):\n",
        "            # Calculamos la salida\n",
        "            salida = self.weights * X[fila]\n",
        "            salida = salida.sum() + self.bias\n",
        "            # Obtenemos el error de entrenamiento\n",
        "            error = y[fila] - salida\n",
        "            error_filas.append(error**2) # Insertamos el error al cuadrado de cada fila para luego obtener el MSE de entrenamiento\n",
        "            # Actualizamos los pesos\n",
        "            self.weights += self.learning_rate * error * X[fila]\n",
        "            # Actualizamos el umbral\n",
        "            self.bias += self.learning_rate * error\n",
        "\n",
        "          # Obtenemos el MSE y lo insertamos en la lista del error de entrenamiento\n",
        "          self.error_entrenamiento.append(np.array(error_filas).mean())\n",
        "\n",
        "          # Para cada fila del conjunto de validación\n",
        "          for fila in range(X_validation.shape[0]):\n",
        "            # Calculamos la salida\n",
        "            salida_validation = self.weights * X_validation[fila]\n",
        "            salida_validation = salida_validation.sum() + self.bias\n",
        "            # Obtenemos el error de validación\n",
        "            error_validation_fila = y_validation[fila] - salida_validation\n",
        "            error_filas_validation.append(error_validation_fila**2) # Insertamos el error al cuadrado de cada fila para luego obtener el MSE de validación\n",
        "\n",
        "          error_validation = np.array(error_filas_validation).mean() # Obtenemos la media de los errores de validación (MSE)\n",
        "          # Guardamos la época con el menor valor de validación\n",
        "          if error_validation < self.best_epoch['MSE Validation']:\n",
        "            self.best_epoch['MSE Validation'] = error_validation\n",
        "            self.best_epoch['Best Epoch'] = i + 1\n",
        "          # Insertamos el MSE de validación en la lista del error de entrenamiento\n",
        "          self.error_validation.append(error_validation)\n",
        "\n",
        "    def predict(self, X):\n",
        "      X = X.values\n",
        "      # Calculamos la salida para cada muestra en X\n",
        "      predictions = []\n",
        "      for fila in range(X.shape[0]):\n",
        "         salida = self.weights * X[fila]\n",
        "         salida = salida.sum() + self.bias\n",
        "         predictions.append(salida)  # Agregamos la salida a la lista de predicciones\n",
        "      return np.array(predictions)\n",
        "\n",
        "\n",
        "    def __initialize_Adaline(self,X):\n",
        "        np.random.seed(self.random_seed)\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.random.uniform(-1,1,size = n_features)\n",
        "        self.bias = np.random.uniform(-1,1)"
      ],
      "metadata": {
        "id": "Q_ZBTuJxxnsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Experimentos realizados\n",
        "A continuación, debemos elegir los mejores hiperparámetros para nuestro conjunto de datos. Para ello, construimos distintos modelos con diferentes hiperparámetros. Escogeremos el modelo con la combinación de hiperparámetros cuyo MSE de validación sea el menor."
      ],
      "metadata": {
        "id": "1LfyfFJ0Uokv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de tasas de aprendizaje a probar\n",
        "learning_rates = [0.1,0.2,0.1,0.05,0.01]\n",
        "# Lista de número de epochs a probar\n",
        "epochs_values = [10,20,30]\n",
        "\n",
        "# Crear un diccionario para almacenar los resultados para crear una tabla\n",
        "results = {\n",
        "    'Learning Rate': [],\n",
        "    'Epochs': [],\n",
        "    'Best Epoch': [],\n",
        "    'MSE Train': [],\n",
        "    'MSE Validation': [],\n",
        "    'MSE Test' : []\n",
        "}\n",
        "\n",
        "# Lista para almacenar los errores MSE de validación\n",
        "mse_validation_values = []\n",
        "mse_train_values = []\n",
        "tiempo_entrenamiento = []\n",
        "\n",
        "# Para cada tasa de aprendizaje\n",
        "for lr in learning_rates:\n",
        "    mse_train_values_lr = []\n",
        "    mse_lr_values = []  # Almacenamos los errores para cada tasa de aprendizaje\n",
        "    tiempo_lr_values = []  # Almacenamos los tiempos para cada tasa de aprendizaje\n",
        "\n",
        "    # Para cada número de épocas\n",
        "    for epochs in epochs_values:\n",
        "        # Creamos el modelo\n",
        "        modelo = Adaline_for(learning_rate=lr, epochs=epochs)\n",
        "        # Entrenamos el modelo y obtenemos el tiempo que ha tardado en entrenarse\n",
        "        start_time = time.time()\n",
        "        modelo.fit(X_train, y_train, X_validation, y_validation)\n",
        "        tiempo_total = time.time() - start_time\n",
        "        tiempo_lr_values.append(tiempo_total) # Guardamos el tiempo\n",
        "\n",
        "        # Obtenemos el MSE de train y validation y lo guardamos\n",
        "        mse_validation = modelo.error_validation\n",
        "        mse_lr_values.append(mse_validation)\n",
        "        mse_train = modelo.error_entrenamiento\n",
        "        mse_train_values_lr.append(mse_train)\n",
        "\n",
        "        # Predecimos los datos de test\n",
        "        prediccion = modelo.predict(X_test)\n",
        "        # Obtenemos el MSE de test\n",
        "        mse_test = mean_squared_error(prediccion, y_test)\n",
        "        print(f'(LR={lr}, Épocas={epochs}) --> Tiempo de entrenamiento {tiempo_total:.2f} s , Best Epoch: {modelo.best_epoch} MSE validación: {mse_validation}, MSE entrenamiento: {mse_train}, MSE test: {mse_test}')\n",
        "\n",
        "        # Almacenamos los resultados en el diccionario\n",
        "        results['Learning Rate'].append(lr)\n",
        "        results['Epochs'].append(epochs)\n",
        "        results['MSE Train'].append(np.min(mse_train))\n",
        "        results['MSE Validation'].append(np.min(mse_validation))\n",
        "        results['MSE Test'].append(mse_test)\n",
        "        results['Best Epoch'].append(modelo.best_epoch['Best Epoch'])\n",
        "\n",
        "    print(\"\\n\")\n",
        "    mse_validation_values.append(mse_lr_values)\n",
        "    mse_train_values.append(mse_train_values_lr)\n",
        "    tiempo_entrenamiento.append(tiempo_lr_values)\n",
        "\n",
        "\n",
        "# CREAMOS LAS GRÁFICAS\n",
        "# Define colores para los gráficos\n",
        "colores = ['b', 'g', 'r', 'y', 'm']\n",
        "# Etiquetas para los errores de validación y entrenamiento\n",
        "etiquetas = ['Validación', 'Entrenamiento']\n",
        "\n",
        "# Iteramos a través de las tasas de aprendizaje\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Iteramos a través de los valores de épocas\n",
        "    for j, epochs in enumerate(epochs_values):\n",
        "        # Configuramos un subplot para el gráfico\n",
        "        plt.subplot(3, 4, j + 1)\n",
        "\n",
        "        # Obtenemos los valores de MSE de validación y entrenamiento para la tasa de aprendizaje y épocas actuales\n",
        "        mse_values_validation = mse_validation_values[i][j]\n",
        "        mse_values_train = mse_train_values[i][j]\n",
        "\n",
        "        # Creamos el gráfico para el error de validación\n",
        "        plt.plot(range(1, len(mse_values_validation) + 1), mse_values_validation, color=colores[i], label=f'LR={lr} (Val)')\n",
        "\n",
        "        # Creamos el gráfico para el error de entrenamiento\n",
        "        plt.plot(range(1, len(mse_values_train) + 1), mse_values_train, linestyle='--', color=colores[i], label=f'LR={lr} (Train)')\n",
        "\n",
        "        # Configuramos las etiquetas y título del gráfico\n",
        "        plt.xlabel('Épocas')\n",
        "        plt.ylabel('MSE')\n",
        "        plt.title(f'MSE (Épocas={epochs}, LR = {lr})')\n",
        "        plt.grid(True)\n",
        "        plt.legend(etiquetas)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# CREAMOS LA TABLA CON LOS RESULTADOS\n",
        "# Creamos un DataFrame a partir del diccionario de resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Seleccionamos las dos últimas columnas y aplicamos formato en notación científica\n",
        "results_df[['MSE Train', 'MSE Validation', 'MSE Test']] = results_df[['MSE Train', 'MSE Validation', 'MSE Test']].applymap(lambda x: f'{x:.2e}')\n",
        "\n",
        "# Creamos una figura\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Creamos los datos de la tabla\n",
        "data = [['Learning Rate', 'Epochs', 'Best Epoch', 'MSE Train', 'MSE Validation', 'MSE Test']]\n",
        "data += list(map(list, results_df.values))\n",
        "\n",
        "# Creamos la tabla en la figura\n",
        "tab = table(ax, cellText=data, loc='center', cellLoc='center', cellColours=[['#ffffff'] * 6] * (len(data)), colWidths=[0.2] * 6)\n",
        "tab.auto_set_font_size(False)\n",
        "tab.set_fontsize(10)\n",
        "tab.scale(1, 1.5)\n",
        "\n",
        "# Ocultamos ejes\n",
        "ax.axis('off')\n",
        "\n",
        "# Mostramos la figura\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tuRw4ukdfNsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Elección del mejor modelo\n",
        "Los hiperparámetros que minimizan el error de validación se corresponden con una tasa de aprendizaje de 0.1 y 4 épocas. Guardamos en un fichero el mejor modelo, así como la evolución del error de entrenamiento y de validación del mismo."
      ],
      "metadata": {
        "id": "YqjtzYwBbMKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el mejor modelo\n",
        "mejor_modelo = Adaline_for(learning_rate=0.1, epochs=4)\n",
        "# Entrenamos de nuevo el modelo\n",
        "mejor_modelo.fit(X_train, y_train, X_validation, y_validation)\n",
        "\n",
        "# Guardamos el mejor modelo en un archivo pickle\n",
        "with open(\"mejor_modelo_Adaline_for.pkl\", \"wb\") as modelo_file:\n",
        "    pickle.dump(mejor_modelo, modelo_file)\n",
        "\n",
        "# Obtenemos el MSE de entrenamiento y validación\n",
        "mse_validation = mejor_modelo.error_validation\n",
        "error_entrenamiento = mejor_modelo.error_entrenamiento\n",
        "\n",
        "# Guardamos los errores de validación y entrenamiento en un archivo CSV\n",
        "errores_df = pd.DataFrame({'Entrenamiento': error_entrenamiento, 'Validacion': mse_validation})\n",
        "errores_df.to_csv('errores_Adaline_for.csv', index=False)"
      ],
      "metadata": {
        "id": "R37hoEouf06J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 Error de test y predicciones\n",
        "Obtenemos el mejor modelo escogido en el apartado anterior. Realizamos las predicciones sobre los datos de test (X_test) y las evaluamos con la variable de respuesta de test (y_test)."
      ],
      "metadata": {
        "id": "0ALKC-h2R-q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el mejor modelo Adaline\n",
        "with open(\"mejor_modelo_Adaline_for.pkl\", \"rb\") as modelo_file:\n",
        "    mejor_modelo_adaline = pickle.load(modelo_file)\n",
        "\n",
        "# Predecimos sobre los datos de test\n",
        "prediccion = mejor_modelo_adaline.predict(X_test)\n",
        "# Obtenemos el MSE\n",
        "mse_test = mean_squared_error(prediccion, y_test)\n",
        "print(f\"MSE de test Adaline: {mse_test:.3e}\")\n",
        "\n",
        "# Desnormalizamos las predicciones y las salidas deseadas\n",
        "prediccion_desnormalizada = prediccion * (y_max - y_min) + y_min\n",
        "y_test_desnormalizada = y_test * (y_max - y_min) + y_min\n",
        "# Obtenemos el MSE en la escala original de los datos\n",
        "mse_test_desnormalizado = mean_squared_error(prediccion_desnormalizada, y_test_desnormalizada)\n",
        "print(f\"MSE de test desnormalizado Adaline: {mse_test_desnormalizado}\")\n",
        "\n",
        "# Creamos un DataFrame con las predicciones y los valores reales\n",
        "data_to_save = pd.DataFrame({'Prediccion': prediccion_desnormalizada.flatten(), 'Valor Real': y_test_desnormalizada})\n",
        "# Ordenamos los valores reales de menor a mayor\n",
        "sorted_data = data_to_save.sort_values(by='Valor Real')\n",
        "# Guardamos el DataFrame en un archivo CSV\n",
        "sorted_data.to_csv('predicciones_y_deseados_Adaline_for.csv', index=False)\n",
        "# Convertimos las columnas en listas de números\n",
        "predicciones = sorted_data['Prediccion'].tolist()\n",
        "valores_reales = sorted_data['Valor Real'].tolist()\n",
        "\n",
        "# Creamos la gráfica\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(range(len(valores_reales)), valores_reales, label='Valor Real', marker='o', s=30, alpha=0.9)\n",
        "plt.scatter(range(len(predicciones)), predicciones, label='Predicción', marker='x', s=30, alpha=0.095)\n",
        "plt.xlabel('Muestras')\n",
        "plt.ylabel('Valores Desnormalizados')\n",
        "plt.title('Comparación entre Valor Real y Predicción (ordenado)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Mostramos la gráfica\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c7UMo6oLfgXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Perceptrón multicapa"
      ],
      "metadata": {
        "id": "c2vd0-rtQsWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Perceptrón multicapa con función de activación RELU"
      ],
      "metadata": {
        "id": "Y4bbBBXuRA1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la función que crea el modelo."
      ],
      "metadata": {
        "id": "R8rvWd-0WJYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape=(X_train.shape [1],) # utilizamos los datos de entrenamiento para definir la tupla , en este caso sera (21,)\n",
        "# Definimos la función para crear el modelo\n",
        "def create_PM_relu(num_hidden_neurons=50):\n",
        "    #1 capa oculta con relu y 1 neurona de salida con sigmoide\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_hidden_neurons, input_shape=input_shape, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "V8JGcR7TSd5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construimos distintos modelos con diferentes hiperparámetros para evaluarlos con los datos de validación y poder escoger el mejor."
      ],
      "metadata": {
        "id": "VXyIj1XSSfzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos una lista de valores para learning rates, número de neuronas ocultas y épocas\n",
        "learning_rates = [0.8, 0.5, 0.4, 0.1, 0.01]\n",
        "hidden_neurons = [10, 20, 30]\n",
        "epochs_list = [10, 15, 20]\n",
        "batch_size = 32\n",
        "\n",
        "# Creamos un diccionario para almacenar los resultados\n",
        "results = {\n",
        "    'Learning Rate': [],\n",
        "    'Epochs': [],\n",
        "    'Best Epoch': [],\n",
        "    'Neuronas': [],\n",
        "    'MSE Train': [],\n",
        "    'MSE Validation': [],\n",
        "    'MSE Test' : []\n",
        "}\n",
        "\n",
        "# Configuramos el diseño de la figura\n",
        "fig, axes = plt.subplots(len(learning_rates), len(epochs_list), figsize=(18, 18))\n",
        "fig.subplots_adjust(hspace=0.5, wspace=0.5)  # Ajustamos el espacio vertical y horizontal\n",
        "\n",
        "# Iteramos sobre las tasas de aprendizaje\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    for k, epochs in enumerate(epochs_list):\n",
        "        # Creamos una paleta de colores\n",
        "        palette = sns.color_palette(\"Set1\", len(hidden_neurons))\n",
        "\n",
        "        # Configuramos la figura para esta tasa de aprendizaje y número de épocas\n",
        "        ax = axes[i, k]\n",
        "\n",
        "        for j, num_neurons in enumerate(hidden_neurons):\n",
        "            model_relu = create_PM_relu(num_neurons)\n",
        "\n",
        "            # CONFIGURAR MODELO Y ENTRENAMIENTO\n",
        "            model_relu.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.SGD(learning_rate=lr, momentum=0), metrics=['mse'])\n",
        "            start_time = time.time()\n",
        "            historico = model_relu.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(X_validation, y_validation), shuffle=False, validation_freq=1)\n",
        "            tiempo_total = time.time() - start_time\n",
        "            print(f\"Tiempo de entrenamiento para (lr = {lr}, epoch = {epochs}): {tiempo_total}s\")\n",
        "            # Obtenemos las predicciones\n",
        "            prediccion_PM = model_relu.predict(X_test)\n",
        "            # Obtenemos el MSE de test\n",
        "            mse_test = mean_squared_error(y_test, prediccion_PM)\n",
        "            print(f\"MSE de test (lr = {lr}, epoch = {epochs}, num_neurons = {num_neurons}): {mse_test:.2e}\")\n",
        "\n",
        "            mse_train = historico.history['loss']\n",
        "            mse_val = historico.history['val_loss']\n",
        "\n",
        "            # Encontramos la época en la que el MSE de validación es el menor\n",
        "            best_epoch = np.argmin(mse_val) + 1\n",
        "\n",
        "            # Obtenemos el valor mínimo del MSE de validación\n",
        "            min_mse_val = np.min(mse_val)\n",
        "\n",
        "            print(f\"Época con el menor MSE de validación: {best_epoch + 1}\")  # Sumamos 1 porque las épocas comienzan desde 0\n",
        "            print(f\"MSE de validación mínimo: {min_mse_val:.2e} \\n\")\n",
        "\n",
        "            # Almacenamos los resultados en el diccionario\n",
        "            results['Learning Rate'].append(lr)\n",
        "            results['Epochs'].append(epochs)\n",
        "            results['Best Epoch'].append(best_epoch)\n",
        "            results['Neuronas'].append(num_neurons)\n",
        "            results['MSE Train'].append(np.min(mse_train))\n",
        "            results['MSE Validation'].append(np.min(mse_val))\n",
        "            results['MSE Test'].append(mse_test)\n",
        "\n",
        "             # Agregamos la gráfica a su subplot correspondiente\n",
        "            ax.plot(historico.history['loss'], label=f'Neurons={num_neurons}', color=palette[j])\n",
        "            ax.plot(historico.history['val_loss'], label=f'Neurons={num_neurons} (Validation)', linestyle='--', color=palette[j])\n",
        "            ax.set_title(f'MSE (LR={lr}, Epochs={epochs})')\n",
        "            ax.set_ylabel('loss')\n",
        "            ax.set_xlabel('epoch')\n",
        "            ax.legend()\n",
        "\n",
        "# PRINTEAMOS LA TABLA\n",
        "# Creamos un DataFrame a partir del diccionario de resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Seleccionamos las dos últimas columnas y aplicamos formato en notación científica\n",
        "results_df[['MSE Train', 'MSE Validation', 'MSE Test']] = results_df[['MSE Train', 'MSE Validation', 'MSE Test']].applymap(lambda x: f'{x:.2e}')\n",
        "\n",
        "# Creamos una figura\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Creamos los datos de la tabla\n",
        "data = [['Learning Rate', 'Epochs', 'Best Epoch', 'Neuronas', 'MSE Train', 'MSE Validation', 'MSE Test']]\n",
        "data += list(map(list, results_df.values))\n",
        "\n",
        "# Creamos la tabla en la figura\n",
        "tab = table(ax, cellText=data, loc='center', cellLoc='center', cellColours=[['#ffffff'] * 7] * (len(data)), colWidths=[0.2] * 7)\n",
        "tab.auto_set_font_size(False)\n",
        "tab.set_fontsize(10)\n",
        "tab.scale(1, 1.5)\n",
        "\n",
        "# Ocultamos ejes\n",
        "ax.axis('off')\n",
        "\n",
        "# Mostramos la figura\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C2hFPaLmTd_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Perceptrón multicapa con función de activación sigmoide"
      ],
      "metadata": {
        "id": "1fKLGkmYRSm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la función para crear el modelo"
      ],
      "metadata": {
        "id": "J0onxl80Vpd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape=(X_train.shape [1],) # utilizamos los datos de entrenamiento para definir la tupla\n",
        "\n",
        "def create_PM_sigmoid(num_hidden_neurons = 50):#se pone un valor por defecto a num_hidden_neurons, pero puede llamarse con otro valor\n",
        "  #1 capa oculta y 1 neurona de salida con sigmoide\n",
        "  model = Sequential() # generamos el modelo dandole una forma secuencial:\n",
        "  model.add(Dense(num_hidden_neurons, input_shape=input_shape, activation='sigmoid')) # capa con x=num_hidden_neurons neuronas activadas con sigmoide\n",
        "  model.add(Dense(1,activation='sigmoid')) # capa con x=1 neurona (solo una neurona de salida) activada con sigmoide\n",
        "  return model"
      ],
      "metadata": {
        "id": "CdCry0JLVxTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construimos distintos modelos con diferentes hiperparámetros para evaluarlos con los datos de validación y poder escoger el mejor."
      ],
      "metadata": {
        "id": "LPkI91XsVy1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos una lista de valores para learning rates, número de neuronas ocultas y épocas\n",
        "learning_rates = [0.8, 0.5, 0.4, 0.1, 0.01]\n",
        "hidden_neurons = [10, 20, 30]\n",
        "epochs_list = [10, 15, 20]\n",
        "batch_size = 32\n",
        "\n",
        "# Creamos un diccionario para almacenar los resultados\n",
        "results = {\n",
        "    'Learning Rate': [],\n",
        "    'Epochs': [],\n",
        "    'Best Epoch': [],\n",
        "    'Neuronas': [],\n",
        "    'MSE Train': [],\n",
        "    'MSE Validation': [],\n",
        "    'MSE Test' : []\n",
        "}\n",
        "\n",
        "# Configuramos el diseño de la figura\n",
        "fig, axes = plt.subplots(len(learning_rates), len(epochs_list), figsize=(18, 18))\n",
        "fig.subplots_adjust(hspace=0.5, wspace=0.5)  # Ajustamos el espacio vertical y horizontal\n",
        "\n",
        "# Iteramos sobre las tasas de aprendizaje\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    for k, epochs in enumerate(epochs_list):\n",
        "        # Creamos una paleta de colores\n",
        "        palette = sns.color_palette(\"Set1\", len(hidden_neurons))\n",
        "\n",
        "        # Configuramos la figura para esta tasa de aprendizaje y número de épocas\n",
        "        ax = axes[i, k]\n",
        "\n",
        "        for j, num_neurons in enumerate(hidden_neurons):\n",
        "            model_sigmoid = create_PM_sigmoid(num_neurons)\n",
        "\n",
        "            # CONFIGURAR MODELO Y ENTRENAMIENTO\n",
        "            model_sigmoid.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.SGD(learning_rate=lr, momentum=0), metrics=['mse'])\n",
        "            start_time = time.time()\n",
        "            historico = model_sigmoid.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(X_validation, y_validation), shuffle=False, validation_freq=1)\n",
        "            tiempo_total = time.time() - start_time\n",
        "            print(f\"Tiempo de entrenamiento para (lr = {lr}, epoch = {epochs}): {tiempo_total}s\")\n",
        "            # Obtenemos las predicciones\n",
        "            prediccion_PM = model_sigmoid.predict(X_test)\n",
        "            # Obtenemos el MSE de test\n",
        "            mse_test = mean_squared_error(y_test, prediccion_PM)\n",
        "            print(f\"MSE de test (lr = {lr}, epoch = {epochs}, num_neurons = {num_neurons}): {mse_test:.2e}\")\n",
        "\n",
        "            mse_train = historico.history['loss']\n",
        "            mse_val = historico.history['val_loss']\n",
        "\n",
        "            # Encontramos la época en la que el MSE de validación es el menor\n",
        "            best_epoch = np.argmin(mse_val)+1\n",
        "\n",
        "            # Obtenemos el valor mínimo del MSE de validación\n",
        "            min_mse_val = np.min(mse_val)\n",
        "\n",
        "            print(f\"Época con el menor MSE de validación: {best_epoch + 1}\")  # Sumamos 1 porque las épocas comienzan desde 0\n",
        "            print(f\"MSE de validación mínimo: {min_mse_val:.2e} \\n\")\n",
        "\n",
        "            # Almacenamos los resultados en el diccionario\n",
        "            results['Learning Rate'].append(lr)\n",
        "            results['Epochs'].append(epochs)\n",
        "            results['Best Epoch'].append(best_epoch)\n",
        "            results['Neuronas'].append(num_neurons)\n",
        "            results['MSE Train'].append(np.min(mse_train))\n",
        "            results['MSE Validation'].append(np.min(mse_val))\n",
        "            results['MSE Test'].append(mse_test)\n",
        "\n",
        "             # Agregamos la gráfica a su subplot correspondiente\n",
        "            ax.plot(historico.history['loss'], label=f'Neurons={num_neurons}', color=palette[j])\n",
        "            ax.plot(historico.history['val_loss'], label=f'Neurons={num_neurons} (Validation)', linestyle='--', color=palette[j])\n",
        "            ax.set_title(f'MSE (LR={lr}, Epochs={epochs})')\n",
        "            ax.set_ylabel('loss')\n",
        "            ax.set_xlabel('epoch')\n",
        "            ax.legend()\n",
        "\n",
        "# PRINTEAMOS LA TABLA\n",
        "# Creamos un DataFrame a partir del diccionario de resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Seleccionamos las dos últimas columnas y aplicamos formato en notación científica\n",
        "results_df[['MSE Train', 'MSE Validation', 'MSE Test']] = results_df[['MSE Train', 'MSE Validation', 'MSE Test']].applymap(lambda x: f'{x:.2e}')\n",
        "\n",
        "# Creamos una figura\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Creamos los datos de la tabla\n",
        "data = [['Learning Rate', 'Epochs', 'Best Epoch', 'Neuronas', 'MSE Train', 'MSE Validation', 'MSE Test']]\n",
        "data += list(map(list, results_df.values))\n",
        "\n",
        "# Creamos la tabla en la figura\n",
        "tab = table(ax, cellText=data, loc='center', cellLoc='center', cellColours=[['#ffffff'] * 7] * (len(data)), colWidths=[0.2] * 7)\n",
        "tab.auto_set_font_size(False)\n",
        "tab.set_fontsize(10)\n",
        "tab.scale(1, 1.5)\n",
        "\n",
        "# Ocultamos ejes\n",
        "ax.axis('off')\n",
        "\n",
        "# Mostramos la figura\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DpmFeZxsbCdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Elección y guardado del mejor modelo\n",
        "Observando los resultados, el mejor modelo es el que se construye utilizando la función de activación RELU con un learning_rate de 0.5, 20 épocas y 20 neuronas ocultas ya que es el que obtiene un MSE de validación más bajo (1.91 e-04). Por tanto, creamos y entrenamos de nuevo un modelo con estos hiperparámetros y lo guardamos.\n",
        "\n",
        "También guardamos su error de entrenamiento y de validación."
      ],
      "metadata": {
        "id": "OAPYQz4KHX_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mejor_modelo = create_PM_relu(30)\n",
        "# CONFIGURAR MODELO Y ENTRENAMIENTO\n",
        "lr = 0.4 # razon de aprendizaje\n",
        "epochs = 20 # numero de ciclos que se quiere realizar en el entrenamiento\n",
        "batch_size=32\n",
        "mejor_modelo.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.SGD(learning_rate=lr, momentum=0), metrics=['mse'] )\n",
        "historico = mejor_modelo.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, validation_data=(X_validation,y_validation),\n",
        "shuffle=False, validation_freq=1)\n",
        "\n",
        "# Obtenemos los errores de validación y entrenamiento del historial\n",
        "historial_entrenamiento = historico.history['loss']\n",
        "historial_validacion = historico.history['val_loss']\n",
        "\n",
        "# Guardamos los errores de validación y entrenamiento en un archivo CSV\n",
        "errores_df = pd.DataFrame({'Entrenamiento': historial_entrenamiento, 'Validacion': historial_validacion})\n",
        "errores_df.to_csv('errores_PM.csv', index=False)\n",
        "\n",
        "# Guardamos el modelo completo\n",
        "mejor_modelo.save('mejor_modelo_PM.keras')"
      ],
      "metadata": {
        "id": "wuWQ1TqiJHlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4 Error de test y predicciones"
      ],
      "metadata": {
        "id": "uK8BsMxRJSI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos el mejor modelo que hemos guardado anteriormente"
      ],
      "metadata": {
        "id": "KVijvgodTrM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos el mejor modelo\n",
        "mejor_modelo_PM = load_model('mejor_modelo_PM.keras')"
      ],
      "metadata": {
        "id": "g3syfWAATrcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluamos el mejor modelo con los datos de test"
      ],
      "metadata": {
        "id": "mv8UQZKKT6Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predecimos sobre los datos de test\n",
        "prediccion_PM = mejor_modelo_PM.predict(X_test)\n",
        "# Obtenemos el MSE de test\n",
        "mse_PM = mean_squared_error(y_test, prediccion_PM)\n",
        "print(f'Error de test: {mse_PM:.3e}')\n",
        "\n",
        "# Desnormalizamos las predicciones y los valores deseados\n",
        "prediccion_desnormalizada_PM = prediccion_PM * (y_max - y_min) + y_min\n",
        "y_test_desnormalizada = y_test * (y_max - y_min) + y_min\n",
        "\n",
        "mse_PM_desnormalizado = mean_squared_error(y_test_desnormalizada, prediccion_desnormalizada_PM)\n",
        "print(f'Error de test desnormalizado: {mse_PM_desnormalizado}')"
      ],
      "metadata": {
        "id": "hnXq1JI0T2gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desnormalizamos las predicciones y las comparamos con los valores deseados."
      ],
      "metadata": {
        "id": "kZT8FNtBUqg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Desnormalizamos las predicciones y los valores deseados\n",
        "prediccion_desnormalizada_PM = prediccion_PM * (y_max - y_min) + y_min\n",
        "y_test_desnormalizada = y_test * (y_max - y_min) + y_min\n",
        "\n",
        "# Convertimos en lista\n",
        "prediccion_desnormalizada_PM = prediccion_desnormalizada_PM.tolist()\n",
        "y_test_desnormalizada = y_test_desnormalizada.tolist()\n",
        "\n",
        "# Creamos un DataFrame con las predicciones y los valores reales\n",
        "data_to_save_PM = pd.DataFrame({'Prediccion': np.array(prediccion_desnormalizada_PM).flatten(), 'Valor Real': y_test_desnormalizada})\n",
        "\n",
        "# Ordenamos los valores reales de menor a mayor\n",
        "sorted_data_PM = data_to_save_PM.sort_values(by='Valor Real')\n",
        "\n",
        "sorted_data_PM.to_csv('predicciones_y_deseados_PM.csv', index=False)\n",
        "\n",
        "# Convertimos las columnas en listas de números\n",
        "predicciones_PM = sorted_data_PM['Prediccion'].tolist()\n",
        "valores_reales = sorted_data_PM['Valor Real'].tolist()\n",
        "\n",
        "# Creamos la gráfica\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(range(len(valores_reales)), valores_reales, label='Valor Real', marker='o', s=30, alpha=0.9)\n",
        "plt.scatter(range(len(predicciones_PM)), predicciones_PM, label='Predicción', marker='x', s=30, alpha=0.095)\n",
        "plt.xlabel('Muestras')\n",
        "plt.ylabel('Valores Desnormalizados')\n",
        "plt.title('Comparación entre Valor Real y Predicción (ordenado)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Mostramos la gráfica\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hGJJGNLKNlOv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}